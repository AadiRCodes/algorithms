import numpy as np
import pdb
from matplotlib import pyplot as plt
from scipy.stats import multivariate_normal as mvn
class GaussianMixture:
    def __init__(self, n_classes, max_iterations, tol) -> None:
        self.n_classes = n_classes
        self.max_iter = max_iterations
        self.tol = tol
        self.means = None
        self.covs = None
        self.weights = None

    def fit(self, X): # X is an N x d matrix, consisting of N samples in Rd.
        N, dim  = np.shape(X)
        self.weights = np.ones(self.n_classes)/(self.n_classes)
        idx = np.random.choice(N, self.n_classes, replace=False)
        self.means = X[idx]
        self.covs = np.array([np.eye(dim) for i in range(self.n_classes)])
        ll = 0
        for c in range(self.max_iter):
            responsibilities = self._calculate_responsibilities(X)
            self._update_parameters(X, responsibilities)



    def _calculate_responsibilities(self, X): # Creates an N x n matrix with entry (i, j) being the probability datapoint i is generated by class j.
        log_probs = np.array([[np.log(mvn.pdf(X[i], self.means[j], self.covs[j]) + 1e-10) for j in range(self.n_classes)] for i in range(len(X))])
        log_probs += np.log(self.weights + 1e-10)  # Adding a small constant for numerical stability
        log_sums = np.log(np.sum(np.exp(log_probs), axis=1) + 1e-10)

        return np.exp(log_probs - log_sums[:, np.newaxis])
        
    def _update_parameters(self, X, resps):
        self.weights = np.mean(resps, axis=0)
        for i in range(self.n_classes):
            self.means[i] = np.dot(resps[:, i], X)/(np.sum(resps[:, i]))
            diffs = X-self.means[i]
            outers = [np.outer(diffs[j], diffs[j]) for j in range(len(X))]
            mats = np.array([resps[j, i]*outers[j] for j in range(len(X))])
            self.covs[i] = np.sum(mats, axis=0)/(np.sum(resps[:, i]))

